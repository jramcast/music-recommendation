%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{\dots} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[sn-mathphys]{sn-jnl}% Math and Physical Sciences Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style
%%\documentclass[sn-standardnature]{sn-jnl}% Standard Nature Portfolio Reference Style
%%\documentclass[default]{sn-jnl}% Default
%%\documentclass[default,iicol]{sn-jnl}% Default with double column layout

%%%% Standard Packages
%%<additional latex packages if required can be included here>
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published
%%%%  by Springer Nature. The guidance has been prepared in partnership with
%%%%  production teams to conform to Springer Nature technical requirements.
%%%%  Editorial and presentation requirements differ among journal portfolios and
%%%%  research disciplines. You may find sections in this template are irrelevant
%%%%  to your work and are empowered to omit any such section if allowed by the
%%%%  journal you intend to submit to. The submission guidelines and policies
%%%%  of the journal take precedence. A detailed User Manual is available in the
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

\jyear{2023}%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}%
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads




% JAIME: this is to hide font size warnings
\usepackage{anyfontsize}




\begin{document}

\title[Predicting Audio Features with Last.fm Tags]{Predicting Audio Features with Last.fm Tags}

%%=============================================================%%
%% Prefix	-> \pfx{Dr}
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% NatureName	-> \tanm{Poet Laureate} -> Title after name
%% Degrees	-> \dgr{MSc, PhD}
%% \author*[1,2]{\pfx{Dr} \fnm{Joergen W.} \spfx{van der} \sur{Ploeg} \sfx{IV} \tanm{Poet Laureate}
%%                 \dgr{MSc, PhD}}\email{iauthor@gmail.com}
%%=============================================================%%

\author*[1]{\fnm{Jaime} \sur{Ramírez Castillo}}\email{Jaime.Ramirez@alu.uclm.es}

\author[1]{\fnm{M. Julia} \sur{Flores}}\email{Julia.Flores@uclm.es}
\equalcont{These authors contributed equally to this work.}


\affil[1]{\orgdiv{Departamento de Sistemas Informáticos}, \orgname{Universidad de Castilla-La Mancha}, \orgaddress{\street{Campus universitario s/n}, \city{Albacete}, \postcode{02071}, \country{Spain}}}


%%==================================%%
%% sample for unstructured abstract %%
%%==================================%%

\abstract{
    In this paper, we discuss a number of experiments to analyze the
    suitability of music label representations to predict certain audio features,
    such as danceability, loudness, or acousticness \dots 
}

%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
%
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
%
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
%
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

\keywords{Music information retrieval, Artificial intelligence}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}\label{sec1}

Music information retrieval (MIR) is a field that focuses on the analysis,
processing and knowledge discovery of latent information in music pieces \cite{ramirez2020machine}.

Many of the challenges in this field use audio features, such as valence and energy, to predict other music-related aspects.
For example, some studies have been using these features to predict the probably of a track being a hit [REF].

To make these predictions, researchers have been using machine learning models, specially over the past few years.
Previously, researchers commonly used hand-crafted audio computational models to perform these tasks.

But what if we tried to use these audio features as the predicted values? We would need a set of predictor variables to
acurrately predict the value of audio features.
This premise is the core concept of this article.

The idea is, given a set of tags, to predict a set of audio features that a hypothetic track would exhibit.
Then, we could build a track selection algorithm that selects actual tracks that are the closest to the predicted audio features.
This process could be part of an explainable recommendation pipeline, where users enter a set of tags, and they recieve the predicted audio features, the closest tracks to those features, and the distance values between each track and the predicted features.

In the remainder of the article, we explain the data gathering and preparation process, as well as the data input formats and varios models.
We will explore various models for the same track and provide insights on how accurately the prediction can be, by using only Last.fm tags.

\section{State of the Art}

In recent years, researchers have used Last.fm tags in classification and regression tasks.
Several studies have used Last.fm to predict music sentiment or mood.

% https://core.ac.uk/download/pdf/234915638.pdf

% https://sciendo.com/pdf/10.2478/ausi-2018-0009

% https://archives.ismir.net/ismir2009/paper/000018.pdf

Additionally, the Spotify audio features \dots 

In general, these studies confirm the posibility of extracting knowledge from Last.fm tags.
To the best of our knowledge, no studies have addressed the problem of audio features regression, based solely on Last.fm tags.

\textcolor{red}{To be Continued \dots }

\section{Generating a Dataset}

Before conducting experiments on predicting audio features from tags, we constructed a dataset, by gathering the data from the Last.fm and Spotify APIs.

\subsection{A Single-user Dataset}

Similar to other intelligent systems, recommender systems
must be trained, by using user preference data, to produce
adequate recommendations. For our recommendation framework,
we have leveraged the knowledge discovery potential
of large historical listening logs, gathered from Last.fm.

To characterize the preferences and context of the user, we
have chosen to start with a simple scenario, where just data
from a single user is available. By training our system with
data from a single user, we also want to begin a discussion,
given the following question: Is it possible to train 
recommender systems, and in particular, user-centric systems, by
using a single-user dataset?

To the best of our knowledge, research on user-centric rec-
ommender systems has concentrated its efforts on explain-
able AI Wang et al. [2019], and also user-centered evaluation
of these systems Knijnenburg et al. [2012]. We also argue
that recommender systems that exploit the preferences of a
single user, or a reduced number of users, might as well be
considered as user-centric models.


\subsection{Last.fm}

Last.fm is a online music service for uses to keep track of their music listenting habits.
Last.fm can also be considered as an community where users tag artists, albums, and tracks, according the the own perception of the user.

For nearly two decades, users have been contributing to Last.fm by tagging tracks with arbitrary text labels.
These tags do not necessarily have to be single-worded.
Users often use short sentences to define a song, such as `I like this track`, or `on the beach`.

Community-contributed tags from the Last.fm API.
These tags are text labels that Last.fm users assign
to artists, albums, or tracks. Users apply these tags to
categorize music from their own perspective, which
means that tags do not fit into any structured ontology
or data model. Tags can refer to aspects such as genre,
emotion, or user context.

\subsubsection{Last.fm Tags}
Last.fm uses the term \emph{scrobble} to refer to a single track playback,
in a particular moment. We have queried the Last.fm
API to download the user{'}s scrobble logs, reported from 2007 to 2022.
For each scrobble, we have gathered the following information:

\begin{itemize}
     \item Track playback timestamp.
     \item Track MusicBrainz Identifier (MBID), if exists.
     \item Track name
     \item Artist name
     \item Track tags. If the track does not have any tags assigned,
      then artist tags have been used.
\end{itemize}


For each tag assigned to a track, or an artist, Last.fm includes
a count property to indicate the popularity of the given tag for the track.
Last.fm normalizes this value in the 0-100 range, so the most popular tag for a track can have a
count value of 100.

Users normally listens to their favorite tracks many times,
so the amount of individual tracks listened is much smaller
than the number of track plays. In this case, the amount of
individual tracks listened is about 20,000.

The format is as follows:

\begin{verbatim}
{
      "artist - name": {
            "eletronica": 100,
            "rock": 80,
            "pop": 45,
            "jazz": 0,
            "nu-jazz": 0,
            "country": 0,
            "soul": 0,
      }
}
\end{verbatim}


\subsection{Spotify}

After gathering Last.fm data and identifying the unique
tracks that represent the user music collection, we have
collected Spotify audio features. For each of these individ-
ual tracks, we have downloaded the Spotify audio features
specific to the given track.

The Spotify audio features are numerical values that repre-
sent high-level audio information computed from a specific
track. These values characterize a track, musically speaking,
by measuring relevant musical aspects.

The features provided by the Spotify API are acousticness,
danceability, duration\textunderscore ms, energy, instrumentalness, key,
liveness, loudness, mode, speechiness, tempo, and valence.
Table 1 describes these features. The reader can find further
details about each feature in the Spotify API documentation
4.

A small portion of the tracks do not have features available in
Spotify, so they have been filtered out from our experiments.

After filtering songs that miss Last.fm tags or Spotify audio features,
our dataset contains 14009 samples.

\emph{Track audio features from the Spotify API}. These are
attributes computed from the audio themselves. They
are a way to describe music by using numerical values.
For example, a danceability attribute of 0.95 means
that a particular song is highly suitable for dancing.

\subsection{Last.fm Tags Representations for Training}


\subsubsection{Tabular}
Each tag is a column and each cell contains the popularity value of a tag for a track.
A cell is 0 if a tag is missing for a track.

The number of columns is limited to the top-K tags.

\subsubsection{Tabular Tokens}

Tags are converted to text tokens. Columns represent token positions, and cells contain the token at a particular position, for a track.
To tokenize tags, we have used the GTP2 tokenizer.
Because the tokenizer requires a string as input, we have converted the set of tags for each track into a string.
To \emph{stringify} the tags, we have concatenated tags with multiple strategies:

\begin{itemize}
      \item By including tag popularity: `rock 2, pop 1`.
      \item By repeating tags based on popularity: `rock rock, pop`.
      \item By ordering by popularity: `rock, pop`.
\end{itemize}

\subsection{Training Data By Track}

When generating training data by track, the tabular formats present sparsity problems.

For tabular representations, we need to defined a fixed set of columns as tags.
For most of tracks, most columns are `0`.

The sparsity of a matrix is the number of zero-valued elements divided by the total number of elements
(e.g., m * n for an m * n matrix) is called the sparsity of the matrix












\subsection{Formatting Input Data for Predicting From Last.fm Tags}

The input data passed to the XGBoost regressor is formatted in tabular format, as follows:

\begin{itemize}
      \item Given that $Tags_{k}$ is the set of most $k$ frequent Last.fm tags in the user listening history
            and, where each $tag \in Tags_{k}$.
      \item Given that $Audio$ is the set of Spotify audio features, where each $feat \in Audio$.
      \item For each $track$:
      \begin{itemize}
            \item $X_{track},_{tag}$ is the strengh of $tag$ for $track$. This value is in the $0-100$ range.
            \item $y_{track},_{feature}$ is the value of the audio feature $y$ for $track$.
      \end{itemize}
\end{itemize}

An example of this data format is provided in table \ref{tabular_tags_format}.

\begin{table}[h]
      \begin{center}
      \begin{minipage}{\textwidth}
      \caption{Tabular data format for Last.fm tags in XGBoost and Bayesian regressors}\label{tabular_tags_format}%
      \begin{tabular}{@{}lllllll@{}}
      \toprule
      Track                         & $X_{electronic}$ & $X_{ambient}$ & $X_{\dots}$ & $y_{energy}$ & $y_{valence}$ & $y_{\dots}$ \\
      \midrule
      Massive Attack - Blue Lines   & 62               & 6             &  \dots      & 0.496        & 0.947         & \dots  \\
      The Beta Band - Squares       & 40               & 3             &  \dots      & 0.446        & 0.507         & \dots  \\
      \dots                         & \dots            & \dots         &  \dots      & \dots        & \dots         & \dots  \\
      \botrule
      \end{tabular}
      \end{minipage}
      \end{center}
\end{table}

\subsection{Formatting Input Data for Predicting From Tokens}

In this particular case, the $X$ values of the tabular input data are tokens.
These tokens are obtained from passing the a string of concatenated Last.fm tags through a tokenizer.
The formal definition of this data format is as follows:

\begin{itemize}
      \item Given that $X_l$ is the token vocabulary, where $l$ is the maximum vocabulary length.
      \item Given that $Audio$ is the set of Spotify audio features, where each $feat \in Audio$.
      \item For each $track$:
      \begin{itemize}
            \item $X_{track},_{n}$ is token found at position $n$, after tokenizing the tags string.
            \item $y_{track},_{feature}$ is the value of the audio feature $y$ for $track$.
      \end{itemize}
\end{itemize}

An example of this data format is provided in table \ref{tabular_token_format}.

\begin{table}[h]
      \begin{center}
      \begin{minipage}{\textwidth}
      \caption{Tabular data format for tokens in XGBoost and Bayesian regressors}\label{tabular_token_format}%
      \begin{tabular}{@{}llllllll@{}}
      \toprule
      Track                         & $X_{0}$ & $X_{1}$ & $X_{2}$ & $X_{\dots}$ & $y_{energy}$ & $y_{valence}$ & $y_{\dots}$ \\
      \midrule
      Massive Attack - Blue Lines   & 101     & 5099    & 6154    &  \dots      & 0.496        & 0.947         &  \dots  \\
      The Beta Band - Squares       & 101     & 4522    & 2600    &  \dots      & 0.446        & 0.507         &  \dots  \\
      \dots                         & \dots   & \dots   & \dots   &  \dots      & \dots        & \dots         &  \dots  \\
      \botrule
      \end{tabular}
      \end{minipage}
      \end{center}
\end{table}


\subsection{Formatting Input Data for Predicting From Text}

When using transformer models, the input data is a string.
We must represent the Last.fm tags, which are initially in the $(tag name, tag popularity)$ form, to a a string.

After converting to a string, the formal definition of the input data is as follows:

\begin{itemize}
      \item Given that $X$ is tags represented as text.
      \item Given that $Audio$ is the set of Spotify audio features, where each $feat \in Audio$.
      \item For each $track$:
      \begin{itemize}
            \item $X_{track},_{n}$ is set of tags for $track$, encoded as a single string.
            \item $y_{track},_{feature}$ is the value of the audio feature $y$ for $track$.
      \end{itemize}
\end{itemize}

An example of this data format is provided in table \ref{text_format}.

\begin{table}[h]
      \begin{center}
      \begin{minipage}{\textwidth}
      \caption{Text data format for tokens in XGBoost and Bayesian regressors}\label{text_format}%
      \begin{tabular}{@{}lllll@{}}
      \toprule
      Track                         & $X$                                   & $y_{energy}$ & $y_{valence}$ & $y_{\dots}$ \\
      \midrule
      Massive Attack - Blue Lines   & "hip hop, chill, bristol, \dots"      & 0.496        & 0.947         & \dots  \\
      The Beta Band - Squares       & "alternative rock, folk, \dots"       & 0.446        & 0.507         & \dots \\
      \dots                         & \dots                                 & \dots        & \dots         & \dots  \\
      \botrule
      \end{tabular}
      \end{minipage}
      \end{center}
\end{table}


















\section{Experiments}

We trained a commonly used machine learning models to predict an audio feature, given the set of tags for a particular track.

\begin{itemize}
      \item Boosted tree regressor \cite{xgboost}
      \item Naive Bayes Regressor \cite{bayesian}
      \item Fine-tuned GPT-2 model
\end{itemize}

\subsection{Boosted Tree Regressor}

We configured the boosted tree regressor model with the following training parameters:

\begin{table}[h]
      \begin{center}
      \begin{minipage}{174pt}
      \caption{Training parameters for XGBoost regressor}\label{xgboosttable}%
      \begin{tabular}{@{}llll@{}}
      \toprule
      Parameter               & Value \\
      \midrule
      objective               & reg:squarederror  \\
      base score              & 0.5 \\
      booster                 & gbtree  \\
      colsample bylevel       & 1 \\
      colsample bynode        & 1 \\
      colsample bytree        & 1 \\
      gamma\footnotemark[1]   & 0 \\
      learning rate           & 0.300000012 \\
      max delta step          & 0 \\
      max depth               & 6 \\
      min child weight        & 1 \\
      estimators              & 200  \\
      n jobs                  & 12  \\
      num parallel tree       & 1 \\
      predictor               & auto  \\
      random state            & 0 \\
      reg alpha               & 0 \\
      reg lambda              & 1 \\
      scale pos weight        & 1 \\
      subsample               & 2 \\
      tree method             & auto  \\
      \botrule
      \end{tabular}
      \footnotetext[1]{Minimum loss reduction required to make a further partition on a leaf node of the tree.}
      \end{minipage}
      \end{center}
\end{table}





% \begin{table}[h]
% \begin{center}
% \begin{minipage}{\textwidth}
% \caption{Data format for XGBoost regressor}\label{tab2}
% \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lcccccc@{\extracolsep{\fill}}}
%       \toprule%
%       & \multicolumn{3}{@{}c@{}}{Tags as columns\footnotemark[1]} & \multicolumn{3}{@{}c@{}}{Tokens as columns\footnotemark[2]} \\\cmidrule{2-4}\cmidrule{5-7}%
%       Track & rock &  \dots  & $\y_{valence}$ & $\position_{0} &  \dots $ & $\sigma_{valence}$ \\
%       \midrule
%       Element 3  & 100 &  \dots  & 0.563 & 780 A & 1166 & $1239\pm100$\\
%       Element 4  & 75  &  \dots  & 0.434 & 900 A & 1268 & $1092\pm40$\\
%       \botrule
%       \end{tabular*}
%       \footnotetext{Note: This is an example of table footnote. This is an example of table footnote this is an example of table footnote this is an example of~table footnote this is an example of table footnote.}
%       \footnotetext[1]{Example for a first table footnote.}
%       \footnotetext[2]{Example for a second table footnote.}
% \end{minipage}
% \end{center}
% \end{table}


\section{Conclusions}

\textcolor{red}{TODO}



\section{Acknowledgments}

\textcolor{red}{TODO}



%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl

%% Default %%
%%\input sn-sample-bib.tex%

\end{document}
